{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Dos mejor que una\n"],"metadata":{"id":"lsiiZCyBYe28"}},{"cell_type":"markdown","source":["\n","## Implementad la función de log verosimilitud usando JAX. Definid muestras (independientes) de tamaños 50, 500 y 5000 para una distribución Gausiana con \u0016 y \u001b de vuestra elección. Implementad un algoritmo de descenso de gradiente utilizando JAX para optimizar la log verosimilitud y estimar los parámetros de la distribución explorando la tasa de aprendizaje yel número máximo de iteraciones. Podéis emplear un valor de \u000f para acabar la optimización de 1e-10 e inicializar los valores de los parámetros a un valor razonable que no este ni demasiado lejos, ni demasiado cerca de los valores reales. Comparad el resultado con el cálculo que dan los estimadores teóricos y comentad el resultado.\n","\n"],"metadata":{"id":"3Ih82EosZdgX"}},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","from jax import grad, vmap, jit, random"],"metadata":{"id":"XeVuT_KdgSqi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Apartado d"],"metadata":{"id":"WrQhybB6KDJn"}},{"cell_type":"markdown","source":["### Implementad la funcion de log verosimilitud usando JAX\n"],"metadata":{"id":"nAZ-hhCHJLN2"}},{"cell_type":"code","source":["@jax.jit\n","def logverosimilitud(p,x):\n","  mean, variance = p\n","  return jnp.log(jnp.sqrt(variance*2*jnp.pi)) + 1/(2*variance)*(x - mean)**2\n","\n","def myMean(x):\n","  return sum(x)/len(x)\n","\n","def myVariance(mean, xData):\n","  return myMean([(x-mean)**2 for x in xData])\n"],"metadata":{"id":"WqiKC9PQgXvg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Definid muestras (independientes) de tamaños 50, 500 y 5000 con media y variancia de vuestra eleccion"],"metadata":{"id":"s6zq3V4bJeNq"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","fig = plt.figure(figsize=(10,10))\n","key1, key2, key3 = random.split(random.PRNGKey(0), 3)\n","\n","mean = 0\n","variance = 0.3\n","\n","data50 = mean + (random.normal(key1, (50,)) * variance)\n","data500 =  mean + (random.normal(key2, (500,)) * variance)\n","data5000 =  mean + (random.normal(key3, (5000,)) * variance)"],"metadata":{"id":"AmRMS1qTi2Sk","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1665311760708,"user_tz":-120,"elapsed":3002,"user":{"displayName":"Jaume Baqueró Quesada","userId":"16660170061432081869"}},"outputId":"61a8100c-6c57-46ec-e61c-7e39a8f5d608"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x720 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","source":["### Implementad un algoritmo de descenso gradiente para optimizar la log verosimilitud y estimar los parametros de la distribucion explorando la tasa de aprendizaje y el numero maximo de iteraciones"],"metadata":{"id":"ubgohkGVJtk_"}},{"cell_type":"code","source":["# INTENTO DE IMPLEMENTARLO NS SI ESTA BIEN\n","# 50\n","epsilon = 1e-10\n","grad_LOGV = jit(grad(logverosimilitud))\n","\n","minLoss = 1000000\n","bestParams = jnp.array([0.5,1])\n","for iterations in ( 500, 1000, 1500, 2000):\n","  for lr in (0.1,0.01, 0.001, 0.0001):\n","\n","    paramLOGV=jnp.array([0.5,1])\n","    ploss = vmap(logverosimilitud,in_axes=(None,0))(paramLOGV,data50).mean(0)\n","    for i in range(iterations):\n","      part = vmap(grad_LOGV,in_axes=(None,0))(paramLOGV,data50)\n","      if jnp.isnan(part.mean(0)).any():\n","        print(\"NAN FOUND WITH HYPERPARAMETERS \", iterations, lr)\n","        break\n","      paramLOGV -= (lr * part.mean(0))\n","      loss = vmap(logverosimilitud,in_axes=(None,0))(paramLOGV,data50).mean(0)\n","      if jnp.abs(ploss - loss) < epsilon:\n","        break\n","      ploss = loss\n","    \n","    if loss < minLoss:\n","      minLoss = loss\n","      bestParams = paramLOGV\n","      print(\"MIN LOSS UPDATED \", minLoss, bestParams, \" WITH HYPERPARAMETERS \", iterations, lr)\n","\n","print(minLoss, bestParams)\n","print(\"--FINISHED OPTIMIZING--\")\n","\n","\n","estimated_mean = myMean(data50)\n","optimized_mean = bestParams[0]\n","\n","estimated_var = myVariance(estimated_mean, data50) \n","optimized_var = bestParams[1]\n","\n","\n","\n","print(\"----RESULTS----\")\n","print(\"Estimated mean \", estimated_mean, \" Optimized mean\", optimized_mean )\n","print(\"Estimated variance \", estimated_var, \" Optimized variance\", optimized_var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IW3afsr8bjT","executionInfo":{"status":"ok","timestamp":1665311803890,"user_tz":-120,"elapsed":43185,"user":{"displayName":"Jaume Baqueró Quesada","userId":"16660170061432081869"}},"outputId":"9f6dffd1-96a9-4d02-b3e2-05b579222888"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MIN LOSS UPDATED  0.8036349 [0.04058447 0.6749266 ]  WITH HYPERPARAMETERS  500 0.1\n","MIN LOSS UPDATED  0.31395987 [0.0407847  0.10970535]  WITH HYPERPARAMETERS  500 0.01\n","0.31395987 [0.0407847  0.10970535]\n","--FINISHED OPTIMIZING--\n","----RESULTS----\n","Estimated mean  0.04058447  Optimized mean 0.040784698\n","Estimated variance  0.109705284  Optimized variance 0.10970535\n"]}]},{"cell_type":"code","source":["# INTENTO DE IMPLEMENTARLO NS SI ESTA BIEN\n","# 500\n","epsilon = 1e-10\n","grad_LOGV = jit(grad(logverosimilitud))\n","\n","minLoss = 1000000\n","bestParams = jnp.array([0.5,1])\n","for iterations in ( 500, 1000, 1500, 2000):\n","  for lr in (0.1,0.01, 0.001, 0.0001):\n","\n","    paramLOGV=jnp.array([0.5,1])\n","    ploss = vmap(logverosimilitud,in_axes=(None,0))(paramLOGV,data500).mean(0)\n","    for i in range(iterations):\n","      part = vmap(grad_LOGV,in_axes=(None,0))(paramLOGV,data500)\n","      if jnp.isnan(part.mean(0)).any():\n","        print(\"NAN FOUND WITH HYPERPARAMETERS \", iterations, lr)\n","        break\n","      paramLOGV -= (lr * part.mean(0))\n","      loss = vmap(logverosimilitud,in_axes=(None,0))(paramLOGV,data500).mean(0)\n","      if jnp.abs(ploss - loss) < epsilon:\n","        break\n","      ploss = loss\n","    \n","    if loss < minLoss:\n","      minLoss = loss\n","      bestParams = paramLOGV\n","      print(\"MIN LOSS UPDATED \", minLoss, bestParams, \" WITH HYPERPARAMETERS \", iterations, lr)\n","\n","print(minLoss, bestParams)\n","print(\"--FINISHED OPTIMIZING--\")\n","\n","\n","estimated_mean = myMean(data500)\n","optimized_mean = bestParams[0]\n","\n","estimated_var = myVariance(estimated_mean, data500) \n","optimized_var = bestParams[1]\n","\n","\n","\n","print(\"----RESULTS----\")\n","print(\"Estimated mean \", estimated_mean, \" Optimized mean\", optimized_mean )\n","print(\"Estimated variance \", estimated_var, \" Optimized variance\", optimized_var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u16ELiGab29d","executionInfo":{"status":"ok","timestamp":1665311834372,"user_tz":-120,"elapsed":30500,"user":{"displayName":"Jaume Baqueró Quesada","userId":"16660170061432081869"}},"outputId":"6faf826d-076e-4f20-a96e-c357026ac012"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NAN FOUND WITH HYPERPARAMETERS  500 0.1\n","MIN LOSS UPDATED  0.1683754 [0.00203477 0.0819926 ]  WITH HYPERPARAMETERS  500 0.01\n","NAN FOUND WITH HYPERPARAMETERS  1000 0.1\n","NAN FOUND WITH HYPERPARAMETERS  1500 0.1\n","NAN FOUND WITH HYPERPARAMETERS  2000 0.1\n","0.1683754 [0.00203477 0.0819926 ]\n","--FINISHED OPTIMIZING--\n","----RESULTS----\n","Estimated mean  0.0018650973  Optimized mean 0.002034775\n","Estimated variance  0.0819925  Optimized variance 0.081992604\n"]}]},{"cell_type":"code","source":["# INTENTO DE IMPLEMENTARLO NS SI ESTA BIEN\n","# 5000\n","epsilon = 1e-10\n","grad_LOGV = jit(grad(logverosimilitud))\n","\n","minLoss = 1000000\n","bestParams = jnp.array([0.5,1])\n","for iterations in ( 500, 1000, 1500, 2000):\n","  for lr in (0.1,0.01, 0.001, 0.0001):\n","\n","    paramLOGV=jnp.array([0.5,1])\n","    ploss = vmap(logverosimilitud,in_axes=(None,0))(paramLOGV,data5000).mean(0)\n","    for i in range(iterations):\n","      part = vmap(grad_LOGV,in_axes=(None,0))(paramLOGV,data5000)\n","      if jnp.isnan(part.mean(0)).any():\n","        print(\"NAN FOUND WITH HYPERPARAMETERS \", iterations, lr)\n","        break\n","      paramLOGV -= (lr * part.mean(0))\n","      loss = vmap(logverosimilitud,in_axes=(None,0))(paramLOGV,data5000).mean(0)\n","      if jnp.abs(ploss - loss) < epsilon:\n","        break\n","      ploss = loss\n","    \n","    if loss < minLoss:\n","      minLoss = loss\n","      bestParams = paramLOGV\n","      print(\"MIN LOSS UPDATED \", minLoss, bestParams, \" WITH HYPERPARAMETERS \", iterations, lr)\n","\n","print(minLoss, bestParams)\n","print(\"--FINISHED OPTIMIZING--\")\n","\n","\n","estimated_mean = myMean(data5000)\n","optimized_mean = bestParams[0]\n","\n","estimated_var = myVariance(estimated_mean, data5000) \n","optimized_var = bestParams[1]\n","\n","\n","\n","print(\"----RESULTS----\")\n","print(\"Estimated mean \", estimated_mean, \" Optimized mean\", optimized_mean )\n","print(\"Estimated variance \", estimated_var, \" Optimized variance\", optimized_var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6I1sjGrfcAJU","executionInfo":{"status":"ok","timestamp":1665311878014,"user_tz":-120,"elapsed":43648,"user":{"displayName":"Jaume Baqueró Quesada","userId":"16660170061432081869"}},"outputId":"c31450e4-4860-4454-fc9a-39c488d04ae1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MIN LOSS UPDATED  2.0587652 [-8.9129610e-03  9.6856985e+00]  WITH HYPERPARAMETERS  500 0.1\n","MIN LOSS UPDATED  0.199132 [-0.0086693   0.08719458]  WITH HYPERPARAMETERS  500 0.01\n","0.199132 [-0.0086693   0.08719458]\n","--FINISHED OPTIMIZING--\n","----RESULTS----\n","Estimated mean  -0.0087123765  Optimized mean -0.008669298\n","Estimated variance  0.08719459  Optimized variance 0.08719458\n"]}]},{"cell_type":"markdown","source":["## Apartado e"],"metadata":{"id":"1G9z2_M6KIPN"}},{"cell_type":"markdown","source":["### Escribid la funcion de densidad de probabilidad para una distribucion que sigue la suma de k Gaussianas"],"metadata":{"id":"7vrMqf5RKMxt"}},{"cell_type":"markdown","source":["$F(x) = \\sum_{i=0}^N \\left[ w_iP_i(x)\\right]$\n","\n","Básicamente es la suma de las funciones de densidad, asignando un peso $w$ a cada una tal que $\\sum w_i = 1$"],"metadata":{"id":"9vZA7NpQMU_G"}},{"cell_type":"markdown","source":["# Descenso de gradiente con k Gaussianas"],"metadata":{"id":"KH3AoehaN4Vm"}},{"cell_type":"markdown","source":["### Generad dos muestras (independientes) de igual tamaño (2500) para cada distribucion usando el generador de muestras normales de JAX.\n"],"metadata":{"id":"IF7oKJtMKjBm"}},{"cell_type":"code","source":["#Creo que las muestras han de tener media (longitud en el enunciado) y variancia distintas.\n","key1, key2 = random.split(random.PRNGKey(0), 2)\n","mean1, mean2 = 0, 1\n","var1, var2 = 0.2, 0.6\n","data1 = mean1 + random.normal(key1, (2500,))*var1\n","data2 = mean2 + random.normal(key2, (2500,))*var2"],"metadata":{"id":"EJHp9NeSMyzM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Implementad un algoritmo de descenso de gradiente para optimizar los parametros de las dos distribuciones con la muestra conjunta utilizando la funcion de log verosimilitud de la mezcla de dos gaussianas"],"metadata":{"id":"BbUPFztoK2Ay"}},{"cell_type":"markdown","source":["# Definición de la función de densidad de probabilidad de la mezcla de dos gaussianas."],"metadata":{"id":"KEhwHBR1DpnS"}},{"cell_type":"code","source":["# Completar la expresión\n","def logverosimilitud2(p,x1,x2):\n","  mean1, variance1, mean2, variance2 = p\n","  return .5*logverosimilitud((mean1, variance1), x1) + .5*(logverosimilitud((mean2, variance2), x2))\n","  #return jnp.log(jnp.sqrt(variance1*2*jnp.pi)) + 1/(2*variance1)*(x1 - mean1)**2 + 1/(2*variance1)*(x2 - mean1)**2 "],"metadata":{"id":"3E8xkcIhOLA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 09/10:Estoy intentando definir la función arriba. He puesto las inicializaciones con vmap y comentado las otras.\n","epsilon = 1e-10\n","grad_LOGV2 = jit(grad(logverosimilitud2)) # derivada, mismos parámetros\n","\n","minLoss = 1000000\n","bestParams = jnp.array([0.5,1,0.5,1])\n","for iterations in (  500, 1000, 1500, 2000):\n","  for lr in ( 0.1,0.01, 0.001, 0.0001):\n","\n","    paramLOGV2=jnp.array([0.5,1,0.5,1])\n","    ploss = vmap(logverosimilitud2,in_axes=(None,0,0))(paramLOGV2,data1,data2).mean(0)\n","    #ploss = logverosimilitud2(paramLOGV2,data1,data2)\n","    for i in range(iterations):\n","      part = vmap(grad_LOGV2,in_axes=(None,0,0))(paramLOGV2,data1,data2)\n","      #part = grad_LOGV2(paramLOGV2,data1,data2)\n","      if jnp.isnan(part.mean(0)).any():\n","        print(\"NAN FOUND WITH HYPERPARAMETERS \", iterations, lr)\n","        break\n","      paramLOGV2 -= (lr * part.mean(0))\n","      loss = vmap(logverosimilitud2,in_axes=(None,0, 0))(paramLOGV2,data1, data2).mean(0)\n","      #loss = logverosimilitud2(paramLOGV2,data1,data2)\n","      if jnp.abs(ploss - loss) < epsilon:\n","        print(f\"Converged at iteration: {i} with lr = {lr}\")\n","        break\n","      ploss = loss\n","    \n","    if loss < minLoss:\n","      minLoss = loss\n","      bestParams = paramLOGV2\n","      print(\"MIN LOSS UPDATED \", minLoss, bestParams, \" WITH HYPERPARAMETERS \", iterations, lr)\n","\n","print(minLoss, bestParams)"],"metadata":{"id":"D-xhnOWMNCSK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665313773386,"user_tz":-120,"elapsed":60133,"user":{"displayName":"Jaume Baqueró Quesada","userId":"16660170061432081869"}},"outputId":"1977b578-2a90-4132-9dc0-a08ad7987199"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NAN FOUND WITH HYPERPARAMETERS  500 0.1\n","MIN LOSS UPDATED  0.35237232 [0.00174782 0.03908178 0.966334   0.37224522]  WITH HYPERPARAMETERS  500 0.01\n","NAN FOUND WITH HYPERPARAMETERS  1000 0.1\n","Converged at iteration: 648 with lr = 0.01\n","MIN LOSS UPDATED  0.35217738 [0.00174782 0.03908178 0.9720973  0.35959622]  WITH HYPERPARAMETERS  1000 0.01\n","NAN FOUND WITH HYPERPARAMETERS  1500 0.1\n","Converged at iteration: 648 with lr = 0.01\n","NAN FOUND WITH HYPERPARAMETERS  2000 0.1\n","Converged at iteration: 648 with lr = 0.01\n","0.35217738 [0.00174782 0.03908178 0.9720973  0.35959622]\n"]}]},{"cell_type":"code","source":["estimated_mean1 = myMean(data1)\n","estimated_mean2 = myMean(data2)\n","\n","\n","estimated_var1 = myVariance(estimated_mean1, data1) \n","estimated_var2 = myVariance(estimated_mean2, data2) \n","print(estimated_mean1, estimated_var1)\n","print(estimated_mean2, estimated_var2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYynuZ-LW73e","executionInfo":{"status":"ok","timestamp":1665312553013,"user_tz":-120,"elapsed":2844,"user":{"displayName":"Jaume Baqueró Quesada","userId":"16660170061432081869"}},"outputId":"ac3fd5d5-be2c-49b2-953d-9bb5b902d8ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0017478205 0.039081775\n","0.9729346 0.3588146\n"]}]},{"cell_type":"markdown","source":["Como podemos ver, los parametros optimizados se parecen mucho a los de las muestras. Muy buena señal!"],"metadata":{"id":"oCChDDxGXbnV"}},{"cell_type":"markdown","source":["## Apartado f\n"],"metadata":{"id":"ubBBDr32LCRl"}},{"cell_type":"markdown","source":["Emplear la misma tasa de aprendizaje durante toda la optimización puede evitar que lleguemos realmente cerca del óptimo. Modificad el algoritmo que habéis implementado para que\n","la tasa de aprendizaje vaya atenuándose con el número de iteraciones multiplicándola por un\n","valor cercano a 1 (0,9999, 0.999, 0,99, ..., 0.9). Repetid la optimización de vuestro mejor resultado en el apartado anterior con diferentes atenuaciones. ¿Ha afectado al resultado? ¿Ha\n","afectado al número de iteraciones antes de converger?\n"],"metadata":{"id":"uk-z5skjLHJF"}},{"cell_type":"code","source":["epsilon = 1e-10\n","grad_LOGV2 = jit(grad(logverosimilitud2)) # derivada, mismos parámetros\n","\n","minLoss = 1000000\n","bestParams = jnp.array([0.5,1,0.5,1])\n","for iterations in (  500, 1000, 1500, 2000):\n","  for lr in ( 0.1,0.01, 0.001, 0.0001):\n","\n","    paramLOGV2=jnp.array([0.5,1,0.5,1])\n","    ploss = vmap(logverosimilitud2,in_axes=(None,0,0))(paramLOGV2,data1,data2).mean(0)\n","    lr2 = lr\n","    for i in range(iterations):\n","      part = vmap(grad_LOGV2,in_axes=(None,0,0))(paramLOGV2,data1,data2)\n","      if jnp.isnan(part.mean(0)).any():\n","        print(\"NAN FOUND WITH HYPERPARAMETERS \", iterations, lr)\n","        break\n","      lr2 = lr2*0.999\n","      paramLOGV2 -= (lr2 * part.mean(0))\n","      loss = vmap(logverosimilitud2,in_axes=(None,0, 0))(paramLOGV2,data1, data2).mean(0)\n","      if jnp.abs(ploss - loss) < epsilon:\n","        print(f\"Converged at iteration: {i} with lr = {lr2}\")\n","        break\n","      ploss = loss\n","    \n","    if loss < minLoss:\n","      minLoss = loss\n","      bestParams = paramLOGV2\n","      print(\"MIN LOSS UPDATED \", minLoss, bestParams, \" WITH HYPERPARAMETERS \", iterations, lr)\n","\n","print(minLoss, bestParams)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kXHHKrKBavS6","executionInfo":{"status":"ok","timestamp":1665314534615,"user_tz":-120,"elapsed":64604,"user":{"displayName":"Jaume Baqueró Quesada","userId":"16660170061432081869"}},"outputId":"14eb94e6-3458-45ce-b140-09eab44b6694"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NAN FOUND WITH HYPERPARAMETERS  500 0.1\n","MIN LOSS UPDATED  0.35675296 [0.00174782 0.03908179 0.94720584 0.4334451 ]  WITH HYPERPARAMETERS  500 0.01\n","NAN FOUND WITH HYPERPARAMETERS  1000 0.1\n","Converged at iteration: 945 with lr = 0.0038810734487645833\n","MIN LOSS UPDATED  0.35218042 [0.00174782 0.03908179 0.9714981  0.36047423]  WITH HYPERPARAMETERS  1000 0.01\n","NAN FOUND WITH HYPERPARAMETERS  1500 0.1\n","Converged at iteration: 945 with lr = 0.0038810734487645833\n","NAN FOUND WITH HYPERPARAMETERS  2000 0.1\n","Converged at iteration: 945 with lr = 0.0038810734487645833\n","0.35218042 [0.00174782 0.03908179 0.9714981  0.36047423]\n"]}]},{"cell_type":"markdown","source":["Como podemos comparar con las iteraciones del apartado e, vemos que el número de iteraciones crece, ya que el learning rate decrece en función de la iteración, lo que causa un progreso gradualmente más lento."],"metadata":{"id":"KoMZ8IqNfIfe"}}]}